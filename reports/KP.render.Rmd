---
title: Knapsack Problem
output:
  rmdformats::readthedown:
    toc: 3
  html_document:
    fig_format: png
---

## About

This page contains an automatic benchmark for the C++ library [idol](https://github.com/hlefebvr/idol)
to test its implementation of the [Branch-and-Bound algorithm](https://en.wikipedia.org/wiki/Branch_and_bound) for solving instances of the [Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem).

The results presented here are automatically generated using GitHub Actions and R with Rmarkdown. Note that the experiments themselves are run with GitHub Actions for which the code
is fully public and can be found [here for implementation details](https://github.com/hlefebvr/idol-benchmark-kp) and [here for GitHub Actions configuration](https://github.com/hlefebvr/idol-benchmark-kp/blob/main/.github/workflows/benchmark.yml).

The experiments were conducted on a free GitHub-hosted runner with an `ubuntu-latest` virtual machine with two CPU cores (x86_64), 7 GB of RAM and 14 GB of SSD space (see [hardware specifications here](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners)).

Last automatic run: `r format(Sys.time(), '%d/%m/%y %H:%M:%S')`.

```{r echo = FALSE}
library(rmarkdown)
suppressMessages(library(dplyr))
library(ggplot2)

knitr::opts_chunk$set(
  out.width = "100%"
)

```

## Mathematical models

Let us consider a set of $n$ items. Each item $i$ has a profit $p_i$ which is earned if the item is selected and a weight $w_i$. The knapsack capacity is noted $W$.

$$
  \begin{array}{lll}
    \max \ & \displaystyle \sum_{j=1}^n p_j x_j \\
    \textrm{s.t.} \ & \displaystyle \sum_{j=1}^n w_j x_j \le W \\
    & x_{j} \in \{ 0,1 \} \quad j=1,...,n.
  \end{array}
$$
Here, variable $x_{j}$ equals 1, if and only if, item $i$ is selected in the knapsack. 

## Reading instances

In this section, we start by reading the raw computational results stored in CSV files. Note that these CSV files is stored as an *artifact* on the [GitHub Actions of the hlefebvr/idol-benchmark-kp repository](https://github.com/hlefebvr/idol-benchmark-kp/actions) and can be downloaded without restrictions by clicking on the latest workflow execution named `benchmark` under the section "Artifacts" (also note that artifacts have a life span of 90 days).

### Raw results

All results can be found in `results_KP_idol.csv` file (Note that this file can be obtained by running `cat results_KP_idol__*.csv > results_KP_idol.csv` after having extracted the `.zip` the artifact).

```{r}
results = read.csv("../results_KP_idol.csv", header = FALSE)
colnames(results) = c("tag", "instance", "solver", "node_selection_rule", "branching_rule", "heuristic", "time", "n_nodes", "best_bound", "best_obj")

results$tag = NULL
```

Instances whose time limit have been reached should have an execution time which is exactly the time limit.
This to avoid issues when plotting perofrmance profiles.

```{r}
time_limit = 5 * 60

if (sum(results$time > time_limit) > 0) {
  results[results$time > time_limit,] = time_limit
}
```

Then, we add a column "solver_with_params" which exactly characterizes a given solver, i.e., a solver with its paraneters which were tweaked.

```{r}
results$solver_with_params = paste0(results$solver, ", ", results$node_selection_rule, ",", results$branching_rule, ", ", results$heuristic)
results[results$solver == "external",]$solver_with_params = "external" 
```

```{r}
results = results %>% 
  mutate(n_items = as.integer(gsub(".*n(\\d+).*", "\\1", instance)))
```

All in all, here are the "raw" results we obtained.

```{r, echo = FALSE}
paged_table(results)
```


### Checking results

In this section, we first make sure that every solver reports the same optimal objective value for solved instances. We thus introduce function `compare_results` which takes as input a set of results and two solver names for which the results should be compared. The function returns the list of instances which for which the two methods report different objective values (with a tolerance of $10^{-3}$).

```{r}
compare_results = function (dataset, solver_a, solver_b) {
  
  results_a = dataset[dataset$solver_with_params == solver_a & dataset$time < time_limit,]
  results_b = dataset[dataset$solver_with_params == solver_b & dataset$time < time_limit,]
  
  merged = merge(results_a, results_b, by = c("instance"))
  
  return ( merged[ abs(merged$objective_value.x - merged$objective_value.y) > 1e-3 ,] )
}
```

Then, we compare all solvers together.

```{r}
mismatches = data.frame()

for (solver_with_params in unique(results$solver_with_params)) {
  mismatch = compare_results(results, "external", solver_with_params)
  if (nrow(mismatch) > 0) {
    print(paste0("There was mismatched instances between external solver and ", solver_with_params))
    mismatch = mismatch[,c("instance", "solver_with_params.x", "solver_with_params.y", "status.x", "status.y", "time.x", "time.y", "objective_value.x", "objective_value.y")]
    rownames(mismatch) = NULL
    mismatches = rbind(mismatches, mismatch)
  }
}

paged_table(mismatches)
```


### Some helper functions

In this section, we introduce the function "get_performance" which augments a given data set with the performance of each solver with respect to the best solver.

```{r}
get_performance = function(dataset, 
                           column_instance = "instance",
                           column_criteria = "time",
                           column_solver = "solver",
                           column_output = "performance") {
  
  # Calculate the best time for each instance
  best_times = dataset %>%
    group_by(!!sym(column_instance)) %>%
    mutate(best_time = min(!!sym(column_criteria)))
  
  # Calculate the performance for each combination of instance and solver
  result = best_times %>%
    group_by(instance, !!sym(column_solver)) %>%
    mutate(!!sym(column_output) := !!sym(column_criteria) / best_time) %>%
    ungroup()

  return (result)
}

plot_performance = function(dataset,
                            column_instance = "instance", 
                            column_criteria = "time", 
                            column_solver = "solver", 
                            column_output = "performance") {
  
  perf = get_performance(dataset, column_instance, column_criteria, column_solver, column_output)
  
  result = ggplot(perf, aes(x = performance, color = !!sym(column_solver))) +
    geom_step(stat = "ecdf") +
    labs(
      x = "Performance w.r.t. best solver",
      y = "ECDF",
      title = paste0("ECDF of ", column_criteria ," performance for all solvers")
    ) +
    scale_color_discrete() +
    scale_x_log10() +
    theme_minimal()
  
  return (result)
  
}
```

We can then plot the performance profile of our solvers using ggplot2 (note that we excluded branch-and-bound with breadth-first node selection and branch-and-bound without heuristics since they do not have the same instance set).

```{r}
to_save = plot_performance(results[results$node_selection_rule != "breadth-first" & (results$heuristic != "-" | results$solver == "external"),], column_solver = "solver_with_params")
to_save
ggsave("ecdf.png", plot = to_save, width = 8, height = 4)
```

## Node selection strategies

### On smaller instances

```{r}
results_node_selection_strategies = results[results$solver == "bab" & results$n_items == 50 & results$branching_rule == "most-infeasible" & results$heuristic == "simple-rounding",]
```

```{r}
plot_performance(results_node_selection_strategies, column_criteria = "time", column_solver = "solver_with_params")
plot_performance(results_node_selection_strategies, column_criteria = "n_nodes", column_solver = "solver_with_params")
```
### On larger instances

```{r}
results_node_selection_strategies = results[results$solver == "bab" & results$n_items > 50 & results$branching_rule == "most-infeasible" & results$heuristic == "simple-rounding",]
```

```{r}
plot_performance(results_node_selection_strategies, column_criteria = "time", column_solver = "solver_with_params")
plot_performance(results_node_selection_strategies, column_criteria = "n_nodes", column_solver = "solver_with_params")
```

## Branching rules


```{r}
results_branching_rules = results[results$solver == "bab" & results$node_selection_rule == "best-bound" & results$heuristic == "simple-rounding",]
```

```{r}
plot_performance(results_branching_rules, column_criteria = "time", column_solver = "solver_with_params")
plot_performance(results_branching_rules, column_criteria = "n_nodes", column_solver = "solver_with_params")
```


## Heuristics


```{r}
results_heuristics = results[results$solver == "bab" & results$n_items == 50 & results$node_selection_rule == "best-bound" & results$branching_rule == "most-infeasible",]
```

```{r}
plot_performance(results_heuristics, column_criteria = "time", column_solver = "solver_with_params")
plot_performance(results_heuristics, column_criteria = "n_nodes", column_solver = "solver_with_params")
```


